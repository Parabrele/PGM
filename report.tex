\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}

\usepackage{natbib}

\usepackage[hidelinks]{hyperref}

\usepackage{wrapfig}

\usepackage{url}

\usepackage[capitalize,nameinlink]{cleveref}

\usepackage{subcaption}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}

\widowpenalty=10000
\clubpenalty=10000

\usepackage[a4paper, margin=1.in]{geometry}
\usepackage{stmaryrd}

\newcommand{\E}{\mathbb{E}}

\begin{document}

\title{Probabilistic Principal Component Analysis}
% author 1 : Grégoire DHIMOÏLA, ENS Paris-Saclay, gregoire.dhimoila@ens-paris-saclay.fr
% author 2 : Clément DUMAS, ENS Paris-Saclay, clement.dumas@ens-paris-saclay.fr
\author{Grégoire DHIMOÏLA \\ ENS Paris-Saclay \and Clément DUMAS \\ ENS Paris-Saclay}
\date{March - August 2024}
\maketitle

\section{Introduction}
\label{sec:introduction}

\paragraph{} Dimensionality reduction is a critical tool in data analysis, enabling the simplification of high-dimensional datasets while retaining their essential structure. Among the most widely used methods is Principal Component Analysis (PCA), which identifies directions of maximum variance in the data. However, the original formulation of PCA lacks a probabilistic framework, limiting its application in scenarios involving missing data, denoising, or the need for a generative model. To address these limitations, Probabilistic Principal Component Analysis (PPCA) was proposed \citep{PPCA}, introducing a probabilistic model to dimensionality reduction.

This probabilistic approach facilitates tasks such as sample generation, missing data imputation or more robust denoising. Furthermore, it can model uncertainty in a principled manner, enabling for example outlier detection. Building upon this probabilistic framework, Mixtures of Probabilistic Principal Component Analyzers (MPPCA) \citep{MPPCA} combines the strengths of PPCA with clustering capabilities akin to Gaussian Mixture Models (GMM), enabling the modeling of complex, multimodal data distributions. The original formulation of PCA can not be extended to such mixture models.

In this report, we delve into the theoretical foundations and practical applications of PPCA and MPPCA. We present a revised approach to the EM algorithm introduced by \citet{PPCA} to explicitly handle missing data. We quantify their performance on various tasks such as missing data reconstruction, compression and denoising, comparing them to standard PCA and other baseline methods. Using the real-world dataset MNIST, we also show how PPCA and MPPCA can be used for data visualization.

\section{Principal Component Analysis}
\label{sec:pca}

\paragraph{} Principal Component Analysis (PCA) is a widely used technique for dimensionality reduction. Given a data matrix $\mathbf{X} \in \R^{N \times d}$, PCA aims to find a set of $q$ orthogonal vectors $\mathbf{w}_1, \ldots, \mathbf{w}_q$ such that the variance of the data projected onto these vectors is maximized. These vectors are called the \emph{principal components} of the data.

To find these principal components, PCA uses the Singular Value Decomposition (SVD) of the data matrix $\mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$, where $\mathbf{U} \in \R^{N \times N}$ and $\mathbf{V} \in \R^{d \times d}$ are orthogonal matrices and $\mathbf{\Sigma} \in \R^{N \times d}$ is a diagonal matrix. The principal components are then the columns of $\mathbf{V}$. They correspond to eigenvectors of the empirical covariance matrix. This has the nice property of minimizing the reconstruction error of the data : $\|\mathbf{X} - \mathbf{V}\mathbf{V}^T\mathbf{X}\|_F^2$.

\section{Probabilistic Principal Component Analysis}
\label{sec:ppca}

\subsection{Model}

\paragraph{} Let $\mathbf{X} \in \R^{N \times d}$ be the data matrix, where $N$ is the number of samples and $d$ is the dimension of the data, and $\mathbf{x} \in \mathbf{X}$ be a data point. We assume that the data is generated by the following model:

\begin{equation}
    \label{eq:ppca_model}
    \mathbf{x} = \mathbf{Wz} + \boldsymbol{\mu} + \boldsymbol{\epsilon}
\end{equation}
    
    where $\mathbf{z} \in \R^q$ is the latent variable, $\mathbf{W} \in \R^{d \times q}$ is the \emph{loading matrix}, $\boldsymbol{\mu} \in \R^d$ is the offset of the model and $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$ is isotropic gaussian noise. We assume that the latent variables are independent and identically distributed, following a standard normal distribution : $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, and that the noise is independent of the latent variables.

From \ref{eq:ppca_model} and the assumption on the noise $\epsilon$ we can write the conditional distribution of the data given the latent variables as:

\begin{equation}
    \label{eq:ppca_conditional}
    \mathbf{x} | \mathbf{z} \sim \mathcal{N}(\mathbf{Wz} + \boldsymbol{\mu}, \sigma^2\mathbf{I})
\end{equation}

and derive the marginal distribution of the data as:

\begin{equation}
    \label{eq:ppca_marginal}
    \mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{WW}^T + \sigma^2\mathbf{I})
\end{equation}

Given this model, the data is thus supposed to be Gaussian. To get the latent variables given some observed data, we can use the conditional distribution of the latent variables given the data, which is also Gaussian:

\begin{equation}
    \label{eq:ppca_z_given_x}
    \mathbf{z} | \mathbf{x} \sim \mathcal{N}(\mathbf{M}^{-1}\mathbf{W}^T(\mathbf{x} - \boldsymbol{\mu}), \sigma^2\mathbf{M}^{-1})
\end{equation}

where $\mathbf{M} = \mathbf{W}^T\mathbf{W} + \sigma^2\mathbf{I}$, and take the mean of this distribution as the estimate of $\mathbf{z}$.

\subsection{Maximum likelihood estimation}

\paragraph{} Closed form solutions for the maximum likelihood estimates of the parameters can be derived from this simple Gaussian model, as shown by \citet{PPCA}. The maximum likelihood estimate of the offset $\boldsymbol{\mu}$ is the empirical mean of the data. The maximum likelihood estimate $\mathbf{W}_{ML}$ of the loading matrix $\mathbf{W}$ is the matrix of the $q$ eigenvectors corresponding to the $q$ largest eigenvalues of the empirical covariance matrix $\Sigma$ of the data while $\sigma^2_{ML}$ is the average of the remaining eigenvalues.

\begin{align}
    \label{eq:ppca_mu_ml}
    \boldsymbol{\mu}_{ML} &= \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n \\
    \label{eq:ppca_sigma_ml}
    \sigma^2_{ML} &= \frac{1}{d-q} \sum_{i=q+1}^d \lambda_i\\
    \label{eq:ppca_W_ml}
    \mathbf{W}_{ML} &= \mathbf{U}_q \left(\mathbf{\Lambda}_q - \sigma_{ML}^2\mathbf{I}\right)^{1/2} \mathbf{R}
\end{align}

where $\mathbf{U}_q$ is the matrix of the $q$ eigenvectors of $\Sigma$, corresponding to the $q$ largest eigenvalues in the diagonal matrix $\mathbf{\Lambda}_q$, and $\mathbf{R}$ is an arbitrary orthogonal matrix - most often ignored and set to $\mathbf{I}$. The $\lambda_i$ are the eigenvalues of $\Sigma$, sorted in decreasing order.

\subsection{Expectation-Maximization algorithm}
\label{sec:ppca_em}

\paragraph{} The ability to account for missing data is one of the main advantages of using a probabilistic model instead of the standard PCA.

The algorithm presented here is our own adaptation of the EM algorithm given by \citet{PPCA} to the case of missing data in $\mathbf{X}$. They give this algorithm in the case where no data is missing, then use it in an experiment with missing data without details on how to adapt the algorithm.

For all $\mathbf{x}$ in $\mathbf{X}$, we isolate
\begin{itemize}
    \item the observed entries $\mathbf{x}^{(o)}$,
    \item the missing entries $\mathbf{x}^{(m)}$,
\end{itemize}

The idea is to estimate the latent variable given the observed data and our current best guess for the missing data, then use these to update the estimate of the missing data. Missing data is dealt with as if it was a latent variable.

Latent variables for this algorithm are both the actual latent variables $\mathbf{z}$ and the missing data $\mathbf{x}^{(m)}$, model parameters are $\mathbf{W}$, $\boldsymbol{\mu}$ and $\sigma^2$. We start the E-step with some estimate of the latent variables and the model parameters, which we use to update the values of the latent variables. Then in the M-step, we use the updated latent variables to update the model parameters.

\paragraph{E-step} Use the data $\mathbf{x}$ - made of observed and estimated missing data - to estimate the mean and covariance of the latent variable $\mathbf{z}|\mathbf{x}$:

\begin{align}
    \label{eq:ppca_e_step_E}
    \E[\mathbf{z}|\mathbf{x}] &= \mathbf{M}^{-1}\mathbf{W}^{T}\left(x - \boldsymbol{\mu}\right) \\
    \label{eq:ppca_e_step_Cov}
    \mathbf{Cov}[\mathbf{z}|\mathbf{x}] &= \sigma^2\mathbf{M}^{-1} + \E[\mathbf{z}|\mathbf{x}]\E[\mathbf{z}|\mathbf{x}]^T
\end{align}
    
    where $\mathbf{M} = \mathbf{W}^{T}\mathbf{W} + \sigma^2\mathbf{I}$. The expectation of the latent variable given the observed data can then be used to estimate the missing data:

\begin{equation}
    \label{eq:ppca_e_step_completion}
    \E[\mathbf{x}^{(m)}|\mathbf{x}^{(o)}] = \mathbf{W}^{(m)}\E[\mathbf{z}|\mathbf{x}] + \boldsymbol{\mu}^{(m)}
\end{equation}

\paragraph{M-step} Use these new values to update $\mathbf{W}$, $\boldsymbol{\mu}$ and $\sigma^2$:

\begin{equation}
    \label{eq:ppca_m_step}
    \widetilde{\mathbf{x}} = \left[
        \begin{array}{c}
            \mathbf{x}^{(o)} \\
            \E[\mathbf{x}^{(m)}|\mathbf{x}^{(o)}]
        \end{array}       
    \right]
\end{equation}

modulo some permutation of the entries of $\mathbf{x}$. We can then use the maximization step of the original EM algorithm to update the parameters of the model:

\begin{align}
    \label{eq:ppca_mu}
    \widetilde{\boldsymbol{\mu}} &= \frac{1}{N}\sum_{n=1}^N \widetilde{\mathbf{x}}_n \\
    \label{eq:ppca_W}
    \widetilde{\mathbf{W}} &= \left[
        \sum_{n=1}^N \left(\widetilde{\mathbf{x}}_n - \widetilde{\boldsymbol{\mu}}\right) \E[\mathbf{z}|\mathbf{x}_n]^T
    \right] \left[
        \sum_{n=1}^N \mathbf{Cov}[\mathbf{z}|\mathbf{x}_n]
    \right] \\
    \label{eq:ppca_sigma}
    \sigma^2 &= \frac{1}{Nd}\sum_{n=1}^N \left\{
        \left\| \widetilde{\mathbf{x}}_n - \widetilde{\boldsymbol{\mu}}\right\|^2 - 2\E[\mathbf{z}|\mathbf{x}_n]^T\widetilde{\mathbf{W}}^T\left(\widetilde{\mathbf{x}}_n - \widetilde{\boldsymbol{\mu}}\right) + \text{tr}\left(\mathbf{Cov}[\mathbf{z}|\mathbf{x}_n]\widetilde{\mathbf{W}}^T\widetilde{\mathbf{W}}\right)
    \right\}
\end{align}

\subsection{Mixtures of PPCA}
\label{sec:mppca}

\paragraph{} An advantage of using a probabilistic model is that it can be easily extended to a mixture model, unlike standard PCA. This is done by introducing $M$ PPCA models, each with its own parameters $\mathbf{W}_m$, $\boldsymbol{\mu}_m$ and $\sigma^2_m$, and a mixing coefficient $\pi_m$ for each model. This is very close to a Gaussian Mixture Model.

The EM algorithm for this model, described in \citet{MPPCA}, updates $\pi_m$ and $\boldsymbol{\mu}_m$ exactly as in a GMM formulation, and everything else follows naturally as described in that paper. To include missing data, we use the same modifications as in the single PPCA model described above. In practice, we never use the EM algorithm described for a single PPCA, we instead use a mixture of $M = 1$ components.

A full description of the algorithm is given in \ref{app:mppca_EM}.

\section{Experiments}

\subsection{Missing data}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{original_image.png}
        \caption{Original Image}
        \label{fig:original_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{damaged_image.png}
        \caption{Damaged Image}
        \label{fig:damaged_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{reconstructed_image.png}
        \caption{Reconstructed Image}
        \label{fig:reconstructed_image}
    \end{subfigure}
    \caption{Illustrative example of image reconstruction using PPCA. (a) Original image, (b) Image with 30\% missing pixels, (c) Reconstructed image using PPCA.}
    \label{fig:mnist_reconstruction}
\end{figure}

\paragraph{} In this experiment, we evaluate the capacity of PPCA to reconstruct missing data. We measure the proximity of the reconstructed data to ground truth and compare it to %two baselines.
a baseline consisting of replacing missing data with their mean value. We proceed as follows:

% \paragraph{Baselines} The first naive approach is to replace missing data with their mean value. The second baseline is to compute a covariance matrix as follows :
% $$\Sigma = \sum_{n=1}^N \left(\mathbf{x}_n^{(o)} - \boldsymbol{\mu}^{(o)}\right)\left(\mathbf{x}_n^{(o)} - \boldsymbol{\mu}^{(o)}\right)^T$$

% Each row and column of $\Sigma$ is then normalized using the number of observed entries in the corresponding row or column. We now reconstruct $x$ as follows :

% $$$$

Let $\mathbf{X}$ be the $N \times d$ data matrix, and $\mathbf{M}_p$ be a $N \times d$ binary mask matrix indicating missing values. Each entry of $\mathbf{M}_p$ is set to $0$ with probability $p$. We then compute $\mathbf{X}_{\text{baseline}}$ and $\mathbf{X}_{\text{PPCA}}$ following the baseline and PPCA methods respectively. We evaluate the reconstruction error as the mean squared error between the reconstructed data and the ground truth:

\begin{equation}
    \label{eq:missing_data_error}
    \text{MSE} = \frac{1}{N \cdot d} \sum_{n=1}^N \sum_{i=1}^d \left(\mathbf{X}_{ni} - \mathbf{X}_{\text{reconstructed},ni}\right)^2
\end{equation}

with $\mathbf{X}_{\text{reconstructed}}$ being either $\mathbf{X}_{\text{baseline}}$ or $\mathbf{X}_{\text{PPCA}}$. We repeat this experiment for different proportions of missing data $p$ and use a mixture of 10 PPCA models on the MNIST dataset. We report the results in \Cref{fig:mse_vs_p}. \Cref{fig:mnist_reconstruction} shows an illustrative example of missing value reconstruction using PPCA.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{mse_vs_p.png}
    \caption{Mean Squared Error (MSE) with respect to the proportion of missing data $p$.}
    \label{fig:mse_vs_p}
\end{figure}

\subsection{Outliers}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{likely_image.png}
        \caption{Likely images \\ Likelihood: -1200, -1250}
        \label{fig:likely_image_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{unlikely_image.png}
        \caption{Unlikely images \\ Likelihood: -3000, -3100}
        \label{fig:unlikely_image_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{damaged_image_likelihood.png}
        \caption{Damaged image \\ Likelihood: -5000}
        \label{fig:damaged_image_likelihood}
    \end{subfigure}
    \caption{Likelihood of MNIST images under the mixture of 10 PPCA models. (a) shows images with high likelihood scores, (b) images with low likelihood scores, and (e) is a severely damaged image with added noise.}
    \label{fig:mnist_likelihood}
\end{figure}

\paragraph{} One of the advantages of using a probabilistic model is having the ability to model uncertainty, which can be used for outlier detection. Given a new data point $\mathbf{x}$, we can compute its likelihood under the PPCA model using \Cref{eq:mppca_EM_marginal} - and the corresponding formula in the mixture setting. If the likelihood is sufficiently low - e.g. by setting a threshold - the data point can be considered an outlier.

In our experiments, we evaluate the outlier detection performance of PPCA on a synthetic dataset. We generate a dataset with one cluster along with some outliers. We then fit a PPCA model to the data and compute the log-likelihood of each data point. We report the results in \Cref{fig:outlier_detection}. \Cref{fig:mnist_likelihood} shows the likelihood of MNIST images under a mixture of 10 PPCA models as well as the likelihood of a damaged image.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{outlier_detection.png}
    \caption{Outlier detection using PPCA. The red points are the detected outliers.}
    \label{fig:outlier_detection}
\end{figure}

\subsection{Data visualization}

TODO : Move to appendix, say that it is essentially nothing groundbreaking, same as PCAs and GMMs visualisations.

% Show how PPCA and MPPCA can be used for data visualization, and how it shows that the method captures meaningful structure in the data. Extend to sample generation.

% Means of MPPCAs should correspond to recognizable classes, PCs should correspond to meaningful variations in the data features, and mean + PC should be recognizable samples.

% By sampling from the model, we should be able to plot likely images.

\paragraph{} Data visualization is an important application of PCA. By projecting high-dimensional data onto a lower-dimensional space, we can gain insights into the structure and relationships within the data, and the visualization of the mean and principal components of the model can also provide useful insights. In this section, we demonstrate how PPCA can be used for data visualization and how MPPCA is better for visualizing multimodal data.

\paragraph{} We use the MNIST dataset for this experiment. The MNIST dataset consists of images of handwritten digits, each of size 28x28 pixels. We first fit a PPCA model with 2 principal components to the dataset and project the data onto the 2-dimensional space spanned by these components. The resulting scatter plot is shown in \Cref{fig:ppca_mnist}, where points correspond to images and are colored by their digit label.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{ppca_mnist.png}
    \caption{2D visualization of the MNIST dataset using PPCA. Each point represents an image, colored by its digit label.}
    \label{fig:ppca_mnist}
\end{figure}

\paragraph{} We then fit a mixture of 10 PPCA models (MPPCA) to the dataset, with each component having 10 principal components. We visualise the mean of each component in \Cref{fig:mppca_mnist_mean} and the first principal components in \Cref{fig:mppca_mnist_pc}. We compare it to the same visualisation using a simple PCA in \Cref{fig:pca_mnist}. TODO : analyse the results. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{mppca_mean.png}
        \caption{Mean of each component $\boldsymbol{\mu}_m$}
        \label{fig:mppca_mnist_mean}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{mppca_pc1.png}
        \caption{First principal component $\mathbf{W}_m^1$}
        \label{fig:mppca_mnist_pc1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{mppca_mean_plus_pc1.png}
        \caption{$\boldsymbol{\mu}_m + \mathbf{W}_m^1$}
        \label{fig:mppca_mnist_mean_plus_pc1}
    \end{subfigure}
    \caption{Visualization of the mean, first principal component, and mean + first principal component for each component in the MPPCA model.}
    \label{fig:mppca_mnist}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pca_mean.png}
        \caption{Mean of the data $\boldsymbol{\mu}$}
        \label{fig:pca_mnist_mean}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pca_pc.png}
        \caption{Principal components $\mathbf{W}^C$}
        \label{fig:pca_mnist_pc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pca_mean_plus_pc.png}
        \caption{$\boldsymbol{\mu}_m + \mathbf{W}_m^1$}
        \label{fig:pca_mnist_mean_plus_pc}
    \end{subfigure}
    \caption{Visualization of the mean, principal components, and mean + principal components for a PCA of the MNIST dataset.}
    \label{fig:mppca_mnist}
\end{figure}

\paragraph{} Additionally, we can use the PPCA models to generate new samples, since in this models, latents are supposed to follow standard normal distributions. By sampling from the latent space and transforming the samples back to the original space, we can generate new images that resemble the original data. \Cref{fig:ppca_samples} show examples of images generated using the MPPCA from the previous visualization.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{ppca_samples.png}
    \caption{Images sampled from the MPPCA models.}
    \label{fig:ppca_samples}
\end{figure}

\paragraph{} These generated samples further illustrate the capability of PPCA to model the underlying structure of the data.

\paragraph{} In summary, PPCA and MPPCA are powerful tools for data visualization and sample generation. They provide insights into the structure of high-dimensional data and enable the generation of new samples that follow the original data distribution.

\subsection{Data compression}

% Using PPCA models, we can store data in a lower-dimensional space. Original data size : 28x28xnx8 bits, compressed data size : qmxnx8 bits for a mixture of m PPCA models.

% Plot the reconstruction error as a function of the compression rate for PCA, PPCA and MPPCA.

\paragraph{} Data compression is a key application of dimensionality reduction methods. We evaluate the performance of PPCA and MPPCA on the task of data compression and compare it to standard PCA. We measure the reconstruction error as a function of the compression rate and report the results in \Cref{fig:compression}. We use the MNIST dataset for this experiment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{compression.png}
    \caption{Reconstruction error as a function of the compression rate for PCA, PPCA and MPPCA.}
    \label{fig:compression}
\end{figure}

\subsection{Denoising}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{denoising_illustration.png}
    \caption{Illustrative example of image denoising using PPCA.}
    \label{fig:denoising_illustration}
\end{figure}

\paragraph{} In this experiment, we evaluate the performance of PPCA on the task of data denoising. We compare the performance of these methods to standard PCA and other baseline methods. We proceed as follows:

\begin{itemize}
    \item Generate a noisy version of the MNIST dataset by adding isotropic Gaussian noise to the images.
    \item Fit a PPCA model to the noisy data and reconstruct the images. We use a mixture of 10 PPCA models with a high number of components $q = \frac{d}{2}$.
    \item Measure the reconstruction error and compare it to the baseline methods.
\end{itemize}

We report reconstruction error as a function of the strenght of the noise $\sigma_{noise}$ in \Cref{fig:denoising}. \Cref{fig:denoising_illustration} shows an illustrative example of image denoising using PPCA.

The intuition behind why PPCA is better than PCA on this task is that the denoising capabilities of PCA arise from the fact that low variance directions can be considered as containing noise while all the interesting signal is stored in high variance directions. However, if the noise is full rank, the first principal components will also contain noise. PPCA can separate the noise from the signal by modeling the noise explicitly in the model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{denoising.png}
    \caption{Reconstruction error as a function of the noise strength for PCA, PPCA and a mixture of 10 PPCA on the MNIST dataset.}
    \label{fig:denoising}
\end{figure}

\section{Discussion}
\label{sec:discussion}

\paragraph{} This report has presented a comprehensive overview of Probabilistic Principal Component Analysis (PPCA) and its extension to Mixtures of Probabilistic Principal Component Analyzers (MPPCA). We have presented both the theoretical framework and practical applications of these methods, particularly in handling noisy or missing data, outlier detection and data visualization.

\paragraph{Strengths of PPCA and MPPCA} One of the principal strengths of PPCA is that it extends the utility of standard PCA to applications requiring a probabilistic framework. This probabilistic basis enables PPCA to:
\begin{itemize}
    \item Handle noisy or missing data seamlessly through the explicit modeling of noise and the Expectation-Maximization (EM) algorithm,
    \item Quantify uncertainty in data, which is beneficial for outlier detection and robustness analysis,
    \item Provide a solid foundation for further extensions, such as MPPCA, which can model complex, multimodal data distributions,
    \item Generate new samples from the model.
\end{itemize}

\paragraph{} The flexibility of MPPCA, akin to Gaussian Mixture Models, allows clustering of data while simultaneously reducing dimensionality. This capability is particularly valuable for tasks involving high-dimensional and heterogeneous datasets, as demonstrated in our experiments on MNIST.

\paragraph{Challenges and Limitations} Despite its advantages, PPCA and its mixture-based extension are not without limitations. Key challenges include:
\begin{itemize}
    \item \textbf{Model Assumptions:} The assumption of Gaussianity in PPCA may limit its applicability to non-Gaussian data. Extending the model to accommodate non-Gaussian latent variables could improve its versatility.
    \item \textbf{Parameter Sensitivity:} The choice of the number of principal components to consider $q$ and the number of mixture components $M$ significantly affects performance, necessitating careful model selection and validation.
\end{itemize}

\addcontentsline{toc}{section}{References}
\bibliography{references}
\bibliographystyle{plainnat}

\newpage
\appendix

\section{EM algorithm for MPPCA with missing data}
\label{app:mppca_EM}

\paragraph{} We present here the full EM algorithm for Mixtures of Probabilistic Principal Component Analyzers (MPPCA) with missing data.

Let $\mathbf{X} \in \R^{N \times d}$ be the data matrix, where each $\mathbf{x}_n$ is a data point separated into observed entries $\mathbf{x}_n^{(o)}$ and missing entries $\mathbf{x}_n^{(m)}$. Let $M$ be the number of PPCA models in the mixture, $\mathbf{Z}^m \in \R^{N \times q}$ be the latent variable matrix for the $m$-th model, $\mathbf{W}_m \in \R^{d \times q}$ be the loading matrix, $\boldsymbol{\mu}_m \in \R^d$ be the offset, $\sigma_m^2 \in \R^M$ be the noise variance, and $\pi_m$ be the mixing coefficient for the $m$-th model.

Following the single PPCA model, we write the conditional distribution of the data given the $m$-th model as:

\begin{equation}
    \label{eq:mppca_EM_conditional}
    \mathbf{x} | m \sim \mathcal{N}(\mathbf{W}_m\mathbf{z}^m + \boldsymbol{\mu}_m, \sigma_m^2\mathbf{I})
\end{equation}

\noindent and the overall distribution of the data as:

\begin{equation}
    \label{eq:mppca_EM_marginal}
    p(\mathbf{x}_n) = \sum_{m=1}^M \pi_m p(\mathbf{x}_n | m)
\end{equation}

Let $R_{nm}$ be the posterior responsibility of the $m$-th component for the generation of the $n$-th data point:

\begin{equation}
    \label{eq:mppca_EM_responsibility}
    R_{nm} = \frac{\pi_m p(\mathbf{x}_n | m)}{p(\mathbf{x}_n)}
\end{equation}

Contrary to the single PPCA model, we will use a two stage EM algorithm. If we follow the same procedure as in \cref{sec:ppca_em}, we will get coupled equations for the updates of $\boldsymbol{\mu}_m$ and $\mathbf{W}_m$, which makes the algorithm intractable. Instead, we will use a two stage EM algorithm adapted from \citet{MPPCA}, condensed as follows:

First, update the responsibilities $R_{nm}$ using \Cref{eq:mppca_EM_responsibility}. Then, do the E-step from the single PPCA model to update the estimate of the missing data :

\begin{align}
    \label{eq:mppca_EM_e_step_E}
    \E[\mathbf{z}^m|\mathbf{x}] &= \mathbf{M}_m^{-1}\mathbf{W}_m^{T}\left(x - \boldsymbol{\mu}_m\right) \\
    \label{eq:mppca_EM_e_step_Cov}
    \mathbf{Cov}[\mathbf{z}^m|\mathbf{x}] &= \sigma_m^2\mathbf{M}_m^{-1} + \E[\mathbf{z}^m|\mathbf{x}]\E[\mathbf{z}^m|\mathbf{x}]^T
\end{align}
    
\noindent where $\mathbf{M}_m = \mathbf{W}_m^{T}\mathbf{W}_m + \sigma_m^2\mathbf{I}$. The expectation of the latent variable given the observed data can then be used to estimate the missing data:

\begin{equation}
    \label{eq:mppca_EM_e_step_completion}
    \E[\mathbf{x}_n^{(m)}|\mathbf{x}_n^{(o)}] = \sum_{m=1}^M R_{nm} \left( \mathbf{W}_m^{(m)}\E[\mathbf{z}^m|\mathbf{x}] + \boldsymbol{\mu}_m^{(m)} \right)
\end{equation}

\noindent With this new values for the data, update $\pi_m$ and $\boldsymbol{\mu}_m$ as follows:

\begin{align}
    \label{eq:mppca_EM_pi}
    \pi_m = \frac{1}{N}\sum_{n=1}^N R_{nm}\\
    \label{eq:mppca_EM_mu}
    \boldsymbol{\mu}_m = \frac{\sum_{n=1}^N R_{nm}\mathbf{x}_n}{\sum_{n=1}^N R_{nm}}
\end{align}

\noindent in order to compute the following covariance matrix:

\begin{equation}
    \label{eq:mppca_EM_cov}
    \Sigma_m = \frac{1}{\pi_mN}\sum_{n=1}^N R_{nm}\left(\mathbf{x}_n - \boldsymbol{\mu}_m\right)\left(\mathbf{x}_n - \boldsymbol{\mu}_m\right)^T
\end{equation}

\noindent which can finally be used to update the loading matrix and the noise variance with \Cref{eq:ppca_W_ml,eq:ppca_sigma_ml} respectively.

% Similarly to the single PPCA model, the EM algorithm for MPPCA will work as follows: latent variables are again the actual latent variables $\mathbf{z}$ and the missing data $\mathbf{x}^{(m)}$, while model parameters are all the parameters of the $m$ components. We start the E-step with some estimate of the latent variables and the model parameters, which we use to update the values of the latent variables $\mathbf{Z}$ and the missing values estimate $\mathbf{x}^{(m)}$, along with the posterior responsibilities $R_{ij}$. Then in the M-step, we use the updated latent variables to update the model parameters.

% \paragraph{E-step} First of all, use the full data $x$ - made of observed and estimated missing data - to evaluate $R_{ij}$ using \cref{eq:mppca_EM_responsibility}. Then, estimate the mean and covariance of the latent variable $\mathbf{z}^j|\mathbf{x}$:

% \begin{align}
%     \label{eq:mppca_EM_e_step_E}
%     \E[\mathbf{z}^j|\mathbf{x}] &= \mathbf{M}_j^{-1}\mathbf{W}_j^{T}\left(x - \boldsymbol{\mu}_j\right) \\
%     \label{eq:mppca_EM_e_step_Cov}
%     \mathbf{Cov}[\mathbf{z}^j|\mathbf{x}] &= \sigma_m^2\mathbf{M}_j^{-1} + \E[\mathbf{z}^j|\mathbf{x}]\E[\mathbf{z}^j|\mathbf{x}]^T
% \end{align}
    
%     where $\mathbf{M}_j = \mathbf{W}_j^{T}\mathbf{W}_j + \sigma_m^2\mathbf{I}$. The expectation of the latent variable given the observed data can then be used to estimate the missing data:

% \begin{equation}
%     \label{eq:mppca_EM_e_step_completion}
%     \E[\mathbf{x}^{(m)}|\mathbf{x}^{(o)}] = \mathbf{W}^{(m)}\E[\mathbf{z}^j|\mathbf{x}] + \boldsymbol{\mu}^{(m)}
% \end{equation}

% \paragraph{M-step} Use the updates from the E-step to do the following updates:

% \begin{align}
%     \label{eq:mppca_EM_m_step_pi}
%     \widetilde{\pi}_j &= \frac{1}{n}\sum_{i=1}^n R_{ij} \\
%     \label{eq:mppca_EM_m_step_mu}
%     \widetilde{\boldsymbol{\mu}}_j &= \frac{\sum_{i=1}^n R_{ij}\left(x_i - \widetilde{\boldsymbol{\mu}}_j\E[\mathbf{z}^j|\mathbf{x}]\right)}{\sum_{i=1}^n R_{ij}} \\
%     \label{eq:mppca_EM_m_step_W}
%     \widetilde{\mathbf{W}}_j &= \left[
%         \sum_{i=1}^n R_{ij}\left(\mathbf{x}_i - \widetilde{\boldsymbol{\mu}}_j\right) \E[\mathbf{z}^j|\mathbf{x}_i]^T
%     \right] \left[
%         \sum_{i=1}^n R_{ij}\mathbf{Cov}[\mathbf{z}^j|\mathbf{x}_i]
%     \right]^{-1} \\
%     \label{eq:mppca_EM_m_step_sigma}
%     \widetilde{\sigma}_j^2 &= \frac{1}{d\sum_{i=1}^n R_{ij}} \left\{ \sum_{i=1}^n R_{ij} \left[\left\| \mathbf{x}_i - \widetilde{\boldsymbol{\mu}}_j\right\|^2 - 2\E[\mathbf{z}^j|\mathbf{x}_i]^T\widetilde{\mathbf{W}}_j^T\left(\mathbf{x}_i - \widetilde{\boldsymbol{\mu}}_j\right) + \text{tr}\left(\mathbf{Cov}[\mathbf{z}^j|\mathbf{x}_i]\widetilde{\mathbf{W}}_j^T\widetilde{\mathbf{W}}_j\right)\right]
%     \right\}
% \end{align}

% \noindent where the tilde denotes the updated values of the parameters. Note that \cref{eq:mppca_EM_m_step_mu} and \cref{eq:mppca_EM_m_step_W} are coupled

\end{document}