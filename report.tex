\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}

\usepackage{natbib}

\usepackage[hidelinks]{hyperref}

\usepackage{wrapfig}

\usepackage{url}

\usepackage[capitalize,nameinlink]{cleveref}

\usepackage{subcaption}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}

\widowpenalty=10000
\clubpenalty=10000

\usepackage[a4paper, margin=1.in]{geometry}
\usepackage{stmaryrd}

\newcommand{\E}{\mathbb{E}}

\begin{document}

\title{Probabilistic Principal Component Analysis}
% author 1 : Grégoire DHIMOÏLA, ENS Paris-Saclay, gregoire.dhimoila@ens-paris-saclay.fr
% author 2 : Clément DUMAS, ENS Paris-Saclay, clement.dumas@ens-paris-saclay.fr
\author{Grégoire DHIMOÏLA \\ ENS Paris-Saclay \and Clément DUMAS \\ ENS Paris-Saclay}
\date{March - August 2024}
\maketitle

\section{Introduction}
\label{sec:introduction}

\paragraph{} Dimensionality reduction is a critical tool in data analysis, enabling the simplification of high-dimensional datasets while retaining their essential structure. Among the most widely used methods is Principal Component Analysis (PCA), which identifies directions of maximum variance in the data. However, the original formulation of PCA lacks a probabilistic framework, limiting its application in scenarios involving missing data, denoising, or the need for a generative model. To address these limitations, Probabilistic Principal Component Analysis (PPCA) was proposed \citep{PPCA}, introducing a probabilistic model to dimensionality reduction.

This probabilistic approach facilitates tasks such as sample generation, missing data imputation or more robust denoising. Furthermore, it can model uncertainty in a principled manner, enabling for example outlier detection. Building upon this probabilistic framework, Mixtures of Probabilistic Principal Component Analyzers (MPPCA) \citep{MPPCA} combines the strengths of PPCA with clustering capabilities akin to Gaussian Mixture Models (GMM), enabling the modeling of complex, multimodal data distributions. The original formulation of PCA can not be extended to such mixture models.

In this report, we delve into the theoretical foundations and practical applications of PPCA and MPPCA. We present a revised approach to the EM algorithm introduced by \citet{PPCA} to explicitly handle missing data. We quantify their performance on various tasks such as missing data reconstruction, compression and denoising, comparing them to standard PCA and other baseline methods. Using both real-world datasets, like MNIST and CIFAR-10, we also show how PPCA and MPPCA can be used for data visualization.

\section{Principal Component Analysis}
\label{sec:pca}

\paragraph{} Principal Component Analysis (PCA) is a widely used technique for dimensionality reduction. Given a data matrix $\mathbf{X} \in \R^{n \times d}$, PCA aims to find a set of $q$ orthogonal vectors $\mathbf{w}_1, \ldots, \mathbf{w}_q$ such that the variance of the data projected onto these vectors is maximized. These vectors are called the \emph{principal components} of the data.

To find these principal components, PCA uses the Singular Value Decomposition (SVD) of the data matrix $\mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$, where $\mathbf{U} \in \R^{n \times n}$ and $\mathbf{V} \in \R^{d \times d}$ are orthogonal matrices and $\mathbf{\Sigma} \in \R^{n \times d}$ is a diagonal matrix. The principal components are then the columns of $\mathbf{V}$. They correspond to eigenvectors of the empirical covariance matrix. This has the nice property of minimizing the reconstruction error of the data : $\|\mathbf{X} - \mathbf{V}\mathbf{V}^T\mathbf{X}\|_F^2$.

\section{Probabilistic Principal Component Analysis}
\label{sec:ppca}

\subsection{Model}

\paragraph{} Let $\mathbf{X} \in \R^{n \times d}$ be the data matrix, where $n$ is the number of samples and $d$ is the dimension of the data, and $\mathbf{x} \in \mathbf{X}$ be a data point. We assume that the data is generated by the following model:

\begin{equation}
    \label{eq:ppca_model}
    \mathbf{x} = \mathbf{Wz} + \boldsymbol{\mu} + \boldsymbol{\epsilon}
\end{equation}
    
    where $\mathbf{z} \in \R^q$ is the latent variable, $\mathbf{W} \in \R^{d \times q}$ is the \emph{loading matrix}, $\boldsymbol{\mu} \in \R^d$ is the offset of the model and $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$ is isotropic gaussian noise. We assume that the latent variables are independent and identically distributed, following a standard normal distribution : $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, and that the noise is independent of the latent variables.

From \ref{eq:ppca_model} and the assumption on the noise $\epsilon$ we can write the conditional distribution of the data given the latent variables as:

\begin{equation}
    \label{eq:ppca_conditional}
    \mathbf{x} | \mathbf{z} \sim \mathcal{N}(\mathbf{Wz} + \boldsymbol{\mu}, \sigma^2\mathbf{I})
\end{equation}

and derive the marginal distribution of the data as:

\begin{equation}
    \label{eq:ppca_marginal}
    \mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{WW}^T + \sigma^2\mathbf{I})
\end{equation}

Given this model, the data is thus supposed to be Gaussian. To get the latent variables given some observed data, we can use the conditional distribution of the latent variables given the data, which is also Gaussian:

\begin{equation}
    \label{eq:ppca_z_given_x}
    \mathbf{z} | \mathbf{x} \sim \mathcal{N}(\mathbf{M}^{-1}\mathbf{W}^T(\mathbf{x} - \boldsymbol{\mu}), \sigma^2\mathbf{M}^{-1})
\end{equation}

where $\mathbf{M} = \mathbf{W}^T\mathbf{W} + \sigma^2\mathbf{I}$, and take the mean of this distribution as the estimate of $\mathbf{z}$.

\subsection{Maximum likelihood estimation}

\paragraph{} Closed form solutions for the maximum likelihood estimates of the parameters can be derived from this simple Gaussian model, as shown by \citet{PPCA}. The maximum likelihood estimate of the offset $\boldsymbol{\mu}$ is the empirical mean of the data. The maximum likelihood estimate $\mathbf{W}_{ML}$ of the loading matrix $\mathbf{W}$ is the matrix of the $q$ eigenvectors corresponding to the $q$ largest eigenvalues of the empirical covariance matrix $\Sigma$ of the data while $\sigma^2_{ML}$ is the average of the remaining eigenvalues.

\begin{align}
    \label{eq:ppca_mu_ml}
    \boldsymbol{\mu}_{ML} &= \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i \\
    \label{eq:ppca_W_ml}
    \mathbf{W}_{ML} &= \mathbf{U}_q \left(\mathbf{\Lambda}_q - \sigma^2\mathbf{I}\right)^{1/2} \mathbf{R} \\
    \label{eq:ppca_sigma_ml}
    \sigma^2_{ML} &= \frac{1}{d-q} \sum_{i=q+1}^d \lambda_i
\end{align}

where $\mathbf{U}_q$ is the matrix of the $q$ eigenvectors of $\Sigma$, corresponding to the $q$ largest eigenvalues in the diagonal matrix $\mathbf{\Lambda}_q$, and $\mathbf{R}$ is an arbitrary orthogonal matrix - most often ignored and set to $\mathbf{I}$. The $\lambda_i$ are the eigenvalues of $\Sigma$, sorted in decreasing order.

\subsection{Expectation-Maximization algorithm}

\paragraph{} The ability to account for missing data is one of the main advantages of using a probabilistic model instead of the standard PCA.

The algorithm presented here is our own adaptation of the EM algorithm given by \citet{PPCA} to the case of missing data in $\mathbf{X}$. They give this algorithm in the case where no data is missing, then uses it in an experiment with missing data without details on how to adapt the algorithm.

For all $x$ in $\mathbf{X}$, we isolate
\begin{itemize}
    \item the observed entries $x^{(o)}$, of dimension $d^{(o)}$,
    \item the missing entries $x^{(m)}$,
    \item the corresponding rows of $\mathbf{W}$, and $\boldsymbol{\mu}$ which we denote $\mathbf{W}^{(o)}$, $\boldsymbol{\mu}^{(o)}$ for the observed entries and $\mathbf{W}^{(m)}$, $\boldsymbol{\mu}^{(m)}$ for the missing entries.
\end{itemize}

The idea is to estimate the mean and covariance of the latent variable given the observed data, then use these to estimate the missing data. Finally, we use the reconstructed data to update the parameters of the model as if the missing data was observed. Missing data is dealt with as if it was a latent variable.

\paragraph{E-step} Use only the observed data $x^{(o)}$ to estimate the mean and covariance of the latent variable $\mathbf{z|X^{(o)}}$:

\begin{align}
    \label{eq:ppca_e_step_E}
    \E[\mathbf{z|x^{(o)}}] &= \mathbf{M}^{(o)-1}\mathbf{W}^{(o)T}\left(x^{(o)} - \boldsymbol{\mu}^{(o)}\right) \\
    \label{eq:ppca_e_step_Cov}
    \mathbf{Cov}[\mathbf{z|x^{(o)}}] &= \sigma^2\mathbf{M}^{(o)-1} + \E[\mathbf{z|x^{(o)}}]\E[\mathbf{z|x^{(o)}}]^T
\end{align}
    
    where $\mathbf{M}^{(o)} = \mathbf{W}^{(o)T}\mathbf{W}^{(o)} + \sigma^2\mathbf{I}$. The expectation of the latent variable given the observed data can then be used to estimate the missing data:

\begin{equation}
    \label{eq:ppca_e_step_completion}
    \E[\mathbf{x^{(m)}|x^{(o)}}] = \mathbf{W}^{(m)}\E[\mathbf{z|x^{(o)}}] + \boldsymbol{\mu}^{(m)}
\end{equation}

\paragraph{M-step} Use the full data to update $\mathbf{W}$, $\boldsymbol{\mu}$ and $\sigma^2$:

\begin{equation}
    \label{eq:ppca_m_step}
    \mathbf{x} = \left[
        \begin{array}{c}
            \mathbf{x}^{(o)} \\
            \E[\mathbf{x^{(m)}|x^{(o)}}]
        \end{array}       
    \right]
\end{equation}

modulo some permutation of the entries of $\mathbf{x}$. We can then use the maximization step of the original EM algorithm to update the parameters of the model:

\begin{align}
    \label{eq:ppca_mu}
    \boldsymbol{\mu} &= \frac{1}{n}\sum_{i=1}^n \mathbf{x}_i \\
    \label{eq:ppca_W}
    \mathbf{W} &= \left\{
        \sum_{i=1}^n \left(\mathbf{x}_i - \boldsymbol{\mu}\right) \E[\mathbf{z|x_i^{(o)}}]^T
    \right\} \left(
        \sum_{i=1}^n \mathbf{Cov}[\mathbf{z|x_i^{(o)}}]
    \right) \\
    \label{eq:ppca_sigma}
    \sigma^2 &= \frac{1}{nd}\sum_{i=1}^n \left\{
        \left\| \mathbf{x}_i - \boldsymbol{\mu}\right\|^2 - 2\E[\mathbf{z|x_i^{(o)}}]^T\mathbf{W}^T\left(\mathbf{x}_i - \boldsymbol{\mu}\right) + \text{tr}\left(\mathbf{Cov}[\mathbf{z|x_i^{(o)}}]\mathbf{W}^T\mathbf{W}\right)
    \right\}
\end{align}

\subsection{Mixtures of PPCA}
\label{sec:mppca}

\paragraph{} An advantage of using a probabilistic model is that it can be easily extended to a mixture model, unlike standard PCA. This is done by introducing M PPCA models, each with its own parameters $\mathbf{W}_m$, $\boldsymbol{\mu}_m$ and $\sigma^2_m$, and a mixing coefficient $\pi_m$ for each model. This is very close to a Gaussian Mixture Model.

The EM algorithm for this model, described in \citet{MPPCA}, updates $\pi_m$ and $\boldsymbol{\mu}_m$ exactly as in a GMM formulation, and everything else follows naturally as described in that paper. To include missing data, we use the same modifications as in the single PPCA model described above.

\section{Experiments}

\subsection{Missing data}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{original_image.png}
        \caption{Original Image}
        \label{fig:original_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{damaged_image.png}
        \caption{Damaged Image}
        \label{fig:damaged_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{reconstructed_image.png}
        \caption{Reconstructed Image}
        \label{fig:reconstructed_image}
    \end{subfigure}
    \caption{Illustrative example of image reconstruction using PPCA. (a) Original image, (b) Image with 30\% missing pixels, (c) Reconstructed image using PPCA.}
    \label{fig:mnist_reconstruction}
\end{figure}

\paragraph{} In this experiment, we evaluate the capacity of PPCA to reconstruct missing data. We measure the proximity of the reconstructed data to ground truth and compare it to %two baselines.
a baseline consisting of replacing missing data with their mean value. We proceed as follows:

% \paragraph{Baselines} The first naive approach is to replace missing data with their mean value. The second baseline is to compute a covariance matrix as follows :
% $$\Sigma = \sum_{i=1}^n \left(\mathbf{x}_i^{(o)} - \boldsymbol{\mu}^{(o)}\right)\left(\mathbf{x}_i^{(o)} - \boldsymbol{\mu}^{(o)}\right)^T$$

% Each row and column of $\Sigma$ is then normalized using the number of observed entries in the corresponding row or column. We now reconstruct $x$ as follows :

% $$$$

Let $\mathbf{X}$ be the $n \times d$ data matrix, and $\mathbf{M_p}$ be a $n \times d$ binary mask matrix indicating missing values. Each entry of $\mathbf{M}$ is set to $0$ with probability $p$. We then compute $\mathbf{X}_{\text{baseline}}$ and $\mathbf{X}_{\text{PPCA}}$ following the baseline and PPCA methods respectively. We evaluate the reconstruction error as the mean squared error between the reconstructed data and the ground truth:

\begin{equation}
    \label{eq:missing_data_error}
    \text{MSE} = \frac{1}{n \cdot d} \sum_{i=1}^n \sum_{j=1}^d \left(\mathbf{X}_{ij} - \mathbf{X}_{\text{reconstructed},ij}\right)^2
\end{equation}

with $\mathbf{X}_{\text{reconstructed}}$ being either $\mathbf{X}_{\text{baseline}}$ or $\mathbf{X}_{\text{PPCA}}$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{mse_vs_p.png}
    \caption{Mean Squared Error (MSE) with respect to the proportion of missing data $p$.}
    \label{fig:mse_vs_p}
\end{figure}

\subsection{Outliers}

\subsection{Likelihood of MNIST images}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{likely_image.png}
        \caption{Likely images \\ Likelihood: -1200, -1250}
        \label{fig:likely_image_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{unlikely_image.png}
        \caption{Unlikely images \\ Likelihood: -3000, -3100}
        \label{fig:unlikely_image_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{damaged_image_likelihood.png}
        \caption{Damaged image \\ Likelihood: -5000}
        \label{fig:damaged_image_likelihood}
    \end{subfigure}
    \caption{Likelihood of MNIST images under the mixture of 10 PPCA models. (a) shows images with high likelihood scores, (b) images with low likelihood scores, and (e) is a severely damaged image with added noise.}
    \label{fig:mnist_likelihood}
\end{figure}

\paragraph{} One of the advantages of PPCA is its ability to model uncertainty, which can be used for outlier detection. Given a new data point $\mathbf{x}$, we can compute its likelihood under the PPCA model. If the likelihood is very low, the data point can be considered an outlier.

To compute the likelihood of a new data point $\mathbf{x}$, we use the marginal distribution of the data given by \cref{eq:ppca_marginal}. The log-likelihood of $\mathbf{x}$ is given by:

\begin{equation}
    \label{eq:ppca_log_likelihood}
    \log p(\mathbf{x}) = -\frac{d}{2} \log(2\pi) - \frac{1}{2} \log |\mathbf{C}| - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \mathbf{C}^{-1} (\mathbf{x} - \boldsymbol{\mu})
\end{equation}

where $\mathbf{C} = \mathbf{WW}^T + \sigma^2\mathbf{I}$ is the covariance matrix of the marginal distribution. We can then set a threshold on the log-likelihood to detect outliers. Data points with log-likelihood below this threshold are considered outliers.

In our experiments, we evaluate the outlier detection performance of PPCA on a synthetic dataset. We generate a dataset with two clusters, one of which contains outliers. We then fit a PPCA model to the data and compute the log-likelihood of each data point. We compare the outlier detection performance of PPCA to that of standard PCA and a Gaussian Mixture Model (GMM).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{outlier_detection.png}
    \caption{Outlier detection using PPCA. The red points are the detected outliers.}
    \label{fig:outlier_detection}
\end{figure}

\paragraph{} As shown in \cref{fig:outlier_detection}, PPCA is able to detect outliers effectively by modeling the uncertainty in the data. This demonstrates the advantage of using a probabilistic model for tasks such as outlier detection.

\subsection{Data visualization}

% Show how PPCA and MPPCA can be used for data visualization, and how it shows that the method captures meaningful structure in the data. Extend to sample generation.

\subsection{Data compression}

\subsection{Denoising}

% compare PCA, PPCA, MPPCA, EM

\addcontentsline{toc}{section}{References}
\bibliography{references}
\bibliographystyle{plainnat}

\newpage
\appendix

\section{}
\label{}

\subsection{}
\label{}

\end{document}